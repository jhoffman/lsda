<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Large Scale Detection through Adaptation by jhoffman</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">LSDA</h1>
        <p class="header"></p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/jhoffman/lsda/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/jhoffman/lsda/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/jhoffman/lsda">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/jhoffman">jhoffman</a></p>


      </header>
      <section>
   <h3>   <a name="welcome" class="anchor" href="#welcome"><span class="octicon octicon-link"></span></a>LSDA</h3>

<p>
Welcome. LSDA is a framework for large scale detection through adaptation. We combine adaptation techniques with deep convolutional models to create a fast and effective large scale detection network.
</p>

<!--<p>
	Check out our object detection demo<br>
    <a href="http://demo.lsda.berkeleyvision.org:5001/">
<img class="displayed" src="images/lsda_demo.png" align="middle" width="500" height="400" style="border-style: none">
    </a>
</p>-->
<!--
        <h3>
<a name="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>DDA: Deep Detector Adaptation</h3>

<p>
	A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. 
	Recently, deep convolutional neural networks (CNN) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. 
	Unfortunately, only a small fraction of those labels are available for the detection task. 
	It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. 
	We propose a Deep Detection Adaptation (DDA) algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors.  
Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. 
</p>

<p>
For full details on our method we are releasing an paper on arXiv. A preprint is currently available <a href="http://www.eecs.berkeley.edu/~jhoffman/papers/adaptdet-submission_7-15-14.pdf">here</a>.
</p>

-->

<h3>7.5K Detector Model</h3>
<p>
	We're releasing a 7604 category detector model for use within <a href="http://caffe.berkeleyvision.org/">Caffe</a>. The categories correspond to the 7404 leaf nodes from the <a href="http://image-net.org/">ImageNet</a> dataset, as well as 200 stonger detectors that are available with bounding box data from the <a href="www.image-net.org/challenges/LSVRC/2013/">ILSVRC2013 challenge</a> dataset. 
</p>



<p>
	Download the <a
    href="http://www.cs.stanford.edu/~jhoffman/caffe_nets/rcnn_model7200.mat">pre-trained
    model</a>. Then run the demo script in the folder.
</p>

<!--<p> Coming Soon: We are preparing a fork of Caffe with the ability to run a
7.5K detector in less than a second per image. </p>-->
<p>
We also have a fork of Caffe with the ability to run a 7.5K detector in less
than a second per image. Check out the 
<a href="https://github.com/ronghanghu/caffe/tree/lsda-nips">
fast lsda project page!
</a>
</p>

<!--<pre><code>$ cd your_repo_root/repo_name
$ git fetch origin
$ git checkout gh-pages
</code></pre>
<-->

<h3>
Citing LSDA</h3>
<pre><code>@inproceedings{Hoffman14Lsda,
   Author = {Judy Hoffman and Sergio Guadarrama and Eric Tzeng and 
   		Ronghang Hu and Jeff Donahue and Ross Girshick and 
	   	Trevor Darrell and Kate Saenko},
   Title = { {LSDA}: Large Scale Detection through Adaptation},
   Year  = {2014},
   booktitle = {Neural Information Processing Systems (NIPS)}
}</code></pre>

<!--<p>You can find the paper <a href="http://arxiv.org/abs/1407.5035">
here
</a>.
<br>
-->
<p>
<a href="http://arxiv.org/abs/1407.5035">
<img class="displayed" src="images/lsda_thumb_paper.png" width="400" style="border-style: none">
</a>
</p>

<h3>
<a name="authors-and-contributors" class="anchor" href="#authors-and-contributors"><span class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<table border="0px">
	<tr>
		<td align="center" width="14%" height="100px">
			<a href="https://www.eecs.berkeley.edu/~jhoffman">
        <img src="http://www.eecs.berkeley.edu/~jhoffman/people/judy.jpg" width="70" height="70" style="border-style: none">
      </a>
      <!--<br>-->
      <a href="https://www.eecs.berkeley.edu/~jhoffman">Judy Hoffman</a>
    </td>

        <td width="14%" align="center">
      <a href="https://github.com/sguada/">
        <img src="https://avatars3.githubusercontent.com/u/1766524?v=3&s=400" width="70" height="70" style="border-style: none">
      </a>
      <br>
      <a href="https://github.com/sguada/">Sergio Guadarrama</a>
    </td>

        <td width="14%" align="center">
      <a href="https://github.com/erictzeng">
        <img src="https://avatars0.githubusercontent.com/u/833004?s=460" width="70" height="70" style="border-style: none">
      </a>
      <br>
      <a href="https://github.com/erictzeng">Eric Tzeng</a>
    </td>

    <td width="14%" align="center">
  <a href="http://jeffdonahue.com/">
  <img src="http://openvoc.berkeleyvision.org/images/jeff.jpg" width="70" height="70" style="border-style: none">
  </a>
  <br>
  <a href="http://jeffdonahue.com/">Jeff Donahue</a>
  </td>

      <td width="14%" align="center">
      <a href="http://www.cs.berkeley.edu/~rbg/">
        <img src="https://www.rossgirshick.info/images/rbg-small.jpg" width="70" height="70" style="border-style: none">
      </a>
      <br>
      <a href="https://www.eecs.berkeley.edu/~rgb">Ross Girshick</a>
    </td>

        <td width="14%" align="center">
      <a href="http://www.eecs.berkeley.edu/~trevor">
        <img src="http://www.eecs.berkeley.edu/~trevor/trevordarrell.jpg" width="70" height="70" style="border-style: none">
      </a>
      <br>
      <a href="https://www.eecs.berkeley.edu/~trevor">Trevor Darrell</a>
    </td>
            <td width="14%" align="center">
      <a href="http://www.cs.uml.edu/~saenko">
        <img src="http://vision.cs.uml.edu/images/kate.gif" width="70" height="70" style="border-style: none">
      </a>
      <br>
      <a href="http://www.cs.uml.edu/~saenko">Kate Saenko</a>
    </td>

  </tr>
</table>

<p>This page and the corresponding software is maintained by <a href="http://www.eecs.berkeley.edu/~jhoffman/">Judy Hoffman</a> (<a href="https://github.com/jhoffman">@jhoffman</a>) and <a href="http://www.eecs.berkeley.edu/~sguada/">Sergio Guadarrama</a> (<a href="https://github.com/sguada/">@sguada</a>). The work is supported by the Berkeley vision group and BVLC. </p>


    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		
  </body>
</html>
